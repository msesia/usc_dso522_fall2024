{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d40902",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# DSO 522: Applied Time Series Analysis for Forecasting\n",
    "\n",
    "## Week 6: Time series regression models\n",
    "\n",
    "### Fall 2024\n",
    "\n",
    "#### Instructor: Dr. Matteo Sesia\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/marshall.png\" alt=\"Marshall School of Business\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96fe97",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0369d0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interactive slides\n",
    "\n",
    "These lecture slides are made using an interactive [Jupyter](https://jupyter.org/) notebook, powered by the [RISE](https://rise.readthedocs.io/en/latest/) extension.\n",
    "\n",
    "In the lectures, we will run `R` code in Jupyter, using the `fpp3` package (which you should have already installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131e9a39",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "suppressMessages(library(fpp3))\n",
    "\n",
    "library(repr)\n",
    "options(repr.matrix.max.rows=4)\n",
    "options(repr.plot.width = 8, repr.plot.height = 4, repr.plot.res = 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2da9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The linear model for time series forecasting\n",
    "\n",
    "The basic concept is that we forecast the time series of interest $y$ assuming that it has a linear relationship with other time series $x$.\n",
    "\n",
    "For example, we might wish to forecast monthly sales $y$ using total advertising spend $x$ as a predictor.\n",
    "\n",
    "Or we might forecast daily electricity demand $y$ using temperature $x_1$ and the day of week $x_2$ as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23076c02",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Simple linear regression\n",
    "\n",
    "\n",
    "In the simplest case, the regression model allows for a linear relationship between the forecast variable $y$ and a single predictor variable $x$: \n",
    "\n",
    "$$y_t = \\beta_0 + \\beta_1 x_t + \\varepsilon_t.$$\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/SLRpop1-1.png\" alt=\"Simple linear model\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b070111",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: US consumption expenditure\n",
    "\n",
    "Time series of quarterly percentage changes (growth rates) of real personal consumption expenditure, $y$, and real personal disposable income, $x$, for the US from 1970 Q1 to 2019 Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ec638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6880d1d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: US consumption expenditure\n",
    "\n",
    "Time series of quarterly percentage changes (growth rates) of real personal consumption expenditure, $y$, and real personal disposable income, $x$, for the US from 1970 Q1 to 2019 Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2576958",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  pivot_longer(c(Consumption, Income), names_to=\"Series\") |>\n",
    "  autoplot(value) +\n",
    "  labs(y = \"% change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ac28e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scatter plot of consumption changes against income changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508a00c4",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  ggplot(aes(x = Income, y = Consumption)) +\n",
    "  labs(y = \"Consumption (quarterly % change)\",\n",
    "       x = \"Income (quarterly % change)\") +\n",
    "  geom_point() #+\n",
    "  #geom_smooth(method = \"lm\", se = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8ae04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear model to predict consumption changes given income changes\n",
    "\n",
    "$$\\hat{y}_t=0.54 + 0.27x_t.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13df620b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  model(TSLM(Consumption ~ Income)) |>\n",
    "  report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8fd716",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "When there are two or more predictor variables, the model is called a multiple regression model. The general form of a multiple regression model is \n",
    "\\begin{equation}\n",
    "  y_t = \\beta_{0} + \\beta_{1} x_{1,t} + \\beta_{2} x_{2,t} + \\cdots + \\beta_{k} x_{k,t} + \\varepsilon_t,\n",
    "\\end{equation}\n",
    "\n",
    "The coefficients $\\beta_1, \\ldots, \\beta_k$ measure the effect of each predictor after taking into account the effects of all the other predictors in the model. Thus, the coefficients measure the marginal effects of the predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2689c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: US consumption expenditure\n",
    "\n",
    "Additional predictors may be useful for forecasting US consumption expenditure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af12795",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  select(-Consumption, -Income) |>\n",
    "  pivot_longer(-Quarter) |>\n",
    "  ggplot(aes(Quarter, value, colour = name)) +\n",
    "  geom_line() +\n",
    "  facet_grid(name ~ ., scales = \"free_y\") +\n",
    "  guides(colour = \"none\") +\n",
    "  labs(y=\"% change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966a0ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pairwise scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd87d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  GGally::ggpairs(columns = 2:6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f15f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Assumptions\n",
    "\n",
    "\\begin{equation}\n",
    "  y_t = \\beta_{0} + \\beta_{1} x_{1,t} + \\beta_{2} x_{2,t} + \\cdots + \\beta_{k} x_{k,t} + \\varepsilon_t,\n",
    "\\end{equation}\n",
    "\n",
    "When we use a linear regression model, we are implicitly making some assumptions.\n",
    " - The linear model accurately describes the true relation between the variables\n",
    " \n",
    "Additionally, we make the following assumptions about the errors ($\\epsilon_1, \\ldots, \\epsilon_T$)\n",
    "- mean zero\n",
    "- not autocorrelated\n",
    "- they are unrelated to the predictor variables\n",
    "\n",
    "It is also useful to have the errors being normally distributed with a constant variance in order to easily produce prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e578bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Least squares estimation\n",
    "\n",
    "In practice, of course, we have a collection of observations but we do not know the values of the coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_k$. These need to be estimated from the data.\n",
    "\n",
    "Least squares principle: choose the values of $\\beta_0, \\beta_1, \\ldots, \\beta_k$ that minimise \n",
    "$$\\sum_{t=1}^T \\varepsilon_t^2 = \\sum_{t=1}^T (y_t -\n",
    "  \\beta_{0} - \\beta_{1} x_{1,t} - \\beta_{2} x_{2,t} - \\cdots - \\beta_{k} x_{k,t})^2.$$\n",
    "  \n",
    "  \n",
    "\n",
    "When we refer to the estimated coefficients, we will use the notation $\\hat{\\beta}, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a5b38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: US consumption expenditure\n",
    "\n",
    "A multiple linear regression model for US consumption is \n",
    "$$y_t=\\beta_0 + \\beta_1 x_{1,t}+ \\beta_2 x_{2,t}+ \\beta_3 x_{3,t}+ \\beta_4 x_{4,t}+\\varepsilon_t,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37a6aa5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings)) |>\n",
    "  report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3eff8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fitted values\n",
    "\n",
    "Predictions of $y$ can be obtained by using the estimated coefficients in the regression equation and setting the error term to zero. In general we write, \n",
    "\\begin{equation}\n",
    "  \\hat{y}_t = \\hat\\beta_{0} + \\hat\\beta_{1} x_{1,t} + \\hat\\beta_{2} x_{2,t} + \\cdots + \\hat\\beta_{k} x_{k,t}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "676cc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings)) |>\n",
    "  augment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d04afd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings)) |>\n",
    "  augment() |>\n",
    "  ggplot(aes(x = Quarter)) +\n",
    "  geom_line(aes(y = Consumption, colour = \"Data\")) +\n",
    "  geom_line(aes(y = .fitted, colour = \"Fitted\")) +\n",
    "  labs(y = NULL, title = \"Percent change in US consumption expenditure\") +\n",
    "  scale_colour_manual(values=c(Data=\"black\",Fitted=\"#D55E00\")) +\n",
    "  guides(colour = guide_legend(title = NULL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a22bfd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings)) |>\n",
    "  augment() |>\n",
    "  ggplot(aes(x = Consumption, y = .fitted)) +\n",
    "  geom_point() +\n",
    "  labs(\n",
    "    y = \"Fitted (predicted values)\",\n",
    "    x = \"Data (actual values)\",\n",
    "    title = \"Percent change in US consumption expenditure\"\n",
    "  ) +\n",
    "  geom_abline(intercept = 0, slope = 1, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57570c9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goodness-of-fit\n",
    "\n",
    "A common way to summarise how well a linear regression model fits the data is via the coefficient of determination, or $R^2$. \n",
    "$$R^2 = \\frac{\\sum(\\hat{y}_{t} - \\bar{y})^2}{\\sum(y_{t}-\\bar{y})^2},$$\n",
    "where the summations are over all observations. \n",
    "\n",
    "In simple linear regression, the value of $R^2$ is also equal to the square of the correlation between y and x (provided an intercept has been included).\n",
    "\n",
    "\n",
    "\n",
    "If the predictions are close to the actual values, we would expect $R^2$ to be close to 1.\n",
    "\n",
    "On the other hand, if the predictions are unrelated to the actual values, then $R^2=0$. \n",
    "\n",
    "In all cases, $R^2$ lies between 0 and 1.\n",
    "\n",
    "The $R^2$ value is used frequently, though often incorrectly, in forecasting. The value of $R^2$ will never decrease when adding an extra predictor to the model and this can lead to over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d1963",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: US consumption expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be9b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings)) |>\n",
    "  augment() |>\n",
    "  ggplot(aes(x = Consumption, y = .fitted)) +\n",
    "  geom_point() +\n",
    "  labs(\n",
    "    y = \"Fitted (predicted values)\",\n",
    "    x = \"Data (actual values)\",\n",
    "    title = \"Percent change in US consumption expenditure\"\n",
    "  ) +\n",
    "  geom_abline(intercept = 0, slope = 1, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb285b4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "us_change |>\n",
    "  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings)) |>\n",
    "  report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283697d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Standard error of the regression\n",
    "\n",
    "Another measure of how well the model has fitted the data is the standard deviation of the residuals, which is often known as the “residual standard error”.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\hat{\\sigma}_e=\\sqrt{\\frac{1}{T-k-1}\\sum_{t=1}^{T}{e_t^2}},\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is the number of predictors in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9f950",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the regression model\n",
    "\n",
    "The differences between the observed $y$ values and the corresponding fitted $\\hat{y}$ values are the training-set errors or “residuals” defined as, \n",
    "\\begin{align*}\n",
    "  e_t &= y_t - \\hat{y}_t \\\\\n",
    "      &= y_t - \\hat\\beta_{0} - \\hat\\beta_{1} x_{1,t} - \\hat\\beta_{2} x_{2,t} - \\cdots - \\hat\\beta_{k} x_{k,t}\n",
    "\\end{align*}\n",
    "\n",
    "The residuals have some useful properties including the following two: \n",
    "$$\n",
    "\\sum_{t=1}^{T}{e_t}=0 \\quad\\text{and}\\quad \\sum_{t=1}^{T}{x_{k,t}e_t}=0\\qquad\\text{for all $k$}.\n",
    "$$\n",
    "\n",
    "As a result of these properties, it is clear that the average of the residuals is zero, and that the correlation between the residuals and the observations for the predictor variable is also zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587eedae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ACF plot of residuals\n",
    "\n",
    "When fitting a regression model to time series data, it is common to find autocorrelation in the residuals. In this case, the estimated model violates the assumption of no autocorrelation in the errors, and our forecasts may be inefficient — there is some information left over which should be accounted for in the model in order to obtain better forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc64681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_consMR <- us_change |>\n",
    "  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings))\n",
    "\n",
    "fit_consMR |> gg_tsresiduals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70e194",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Residual plots against predictors\n",
    "\n",
    "We would expect the residuals to be randomly scattered without showing any systematic patterns. \n",
    "\n",
    "A simple and quick way to check this is to examine scatterplots of the residuals against each of the predictor variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9d05425",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_change |> \n",
    "    left_join(augment(fit_consMR)) |>\n",
    "    pivot_longer(Income:Unemployment,\n",
    "               names_to = \"regressor\", values_to = \"x\") |>\n",
    "  ggplot(aes(x = x, y = .resid)) +\n",
    "  geom_point() +\n",
    "  facet_wrap(. ~ regressor, scales = \"free_x\") +\n",
    "  labs(y = \"Residuals\", x = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98829e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Residual plots against fitted values\n",
    "\n",
    "A plot of the residuals against the fitted values should also show no pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde42983",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_consMR |>\n",
    "  augment() |>\n",
    "  ggplot(aes(x = .fitted, y = .resid)) +\n",
    "  geom_point() + labs(x = \"Fitted\", y = \"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79eb48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outliers and influential observations\n",
    "\n",
    "Observations that take extreme values compared to the majority of the data are called *outliers*.\n",
    "\n",
    "Observations that have a large influence on the estimated coefficients of a regression model are called *influential observations*. \n",
    "\n",
    "Usually, influential observations are also outliers that are extreme in the $x$ direction.\n",
    "\n",
    "One source of outliers is incorrect data entry.\n",
    "\n",
    "Outliers also occur when some observations are simply different. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1684c33",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: US consumption expenditure\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/outlier-1.png\" alt=\"Outliers\" width=\"1200\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759ad3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spurious regression\n",
    "\n",
    "More often than not, time series data are “non-stationary”; that is, the values of the time series do not fluctuate around a constant mean or with a constant variance.\n",
    "\n",
    "Different time series may appear to be related simply because they both trend upwards in the same manner.\n",
    "However, they may not be related to one another at all!\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/spurious-1.png\" alt=\"Spurious\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "Cases of spurious regression might appear to give reasonable short-term forecasts, but they will generally not continue to work into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de9ad9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Some useful predictors\n",
    "\n",
    "There are several useful predictors that occur frequently when using regression for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd791037",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Trend\n",
    "\n",
    "It is common for time series data to be trending. A linear trend can be modelled by simply using $x_{1,t}=t$ as a predictor.\n",
    "\n",
    "$$y_{t}= \\beta_0+\\beta_1t+\\varepsilon_t,$$\n",
    "\n",
    "A trend variable can be specified in the `TSLM()` function using the `trend()` special. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ce5dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dummy variables\n",
    "\n",
    "So far, we have assumed that each predictor takes numerical values. But what about when a predictor is a categorical variable taking only two values (e.g., “yes” and “no”)? Such a variable might arise, for example, when forecasting daily sales and you want to take account of whether the day is a public holiday or not. So the predictor takes value “yes” on a public holiday, and “no” otherwise.\n",
    "\n",
    "A dummy variable can also be used to account for an outlier in the data. Rather than omit the outlier, a dummy variable removes its effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cd1e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Seasonal dummy variables\n",
    "\n",
    "Suppose that we are forecasting daily data and we want to account for the day of the week as a predictor. Then the following dummy variables can be created.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/seasonal_dummies.png\" alt=\"Seasonal dummies\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "Notice that only six dummy variables are needed to code seven categories.\n",
    "\n",
    "The interpretation of each of the coefficients associated with the dummy variables is that it is a measure of the effect of that category relative to the omitted category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96701fb6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Australian quarterly beer production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7509316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_production <- aus_production |>\n",
    "  filter(year(Quarter) >= 1992)\n",
    "recent_production |>\n",
    "  autoplot(Beer) +\n",
    "  labs(y = \"Megalitres\",\n",
    "       title = \"Australian quarterly beer production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ef107",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We want to forecast the value of future beer production. We can model this data using a regression model with a linear trend and quarterly dummy variables, \n",
    "$$y_{t} = \\beta_{0} + \\beta_{1} t + \\beta_{2}d_{2,t} + \\beta_3 d_{3,t} + \\beta_4 d_{4,t} + \\varepsilon_{t},$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df029fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_beer <- recent_production |>\n",
    "  model(TSLM(Beer ~ trend() + season()))\n",
    "report(fit_beer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f39eca1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "augment(fit_beer) |>\n",
    "  ggplot(aes(x = Quarter)) +\n",
    "  geom_line(aes(y = Beer, colour = \"Data\")) +\n",
    "  geom_line(aes(y = .fitted, colour = \"Fitted\")) +\n",
    "  scale_colour_manual(\n",
    "    values = c(Data = \"black\", Fitted = \"#D55E00\")\n",
    "  ) +\n",
    "  labs(y = \"Megalitres\",\n",
    "       title = \"Australian quarterly beer production\") +\n",
    "  guides(colour = guide_legend(title = \"Series\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "391d74b8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "augment(fit_beer) |>\n",
    "  ggplot(aes(x = Beer, y = .fitted,\n",
    "             colour = factor(quarter(Quarter)))) +\n",
    "  geom_point() +\n",
    "  labs(y = \"Fitted\", x = \"Actual values\",\n",
    "       title = \"Australian quarterly beer production\") +\n",
    "  geom_abline(intercept = 0, slope = 1) +\n",
    "  guides(colour = guide_legend(title = \"Quarter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91ce1f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What if we do not use seasonal dummy variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9464fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_production |>\n",
    "  model(TSLM(Beer ~ trend())) |>\n",
    "  augment() |>\n",
    "  ggplot(aes(x = Quarter)) +\n",
    "  geom_line(aes(y = Beer, colour = \"Data\")) +\n",
    "  geom_line(aes(y = .fitted, colour = \"Fitted\")) +\n",
    "  scale_colour_manual(\n",
    "    values = c(Data = \"black\", Fitted = \"#D55E00\")\n",
    "  ) +\n",
    "  labs(y = \"Megalitres\",\n",
    "       title = \"Australian quarterly beer production\") +\n",
    "  guides(colour = guide_legend(title = \"Series\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ee5e462",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "recent_production |>\n",
    "  model(TSLM(Beer ~ trend())) |>\n",
    "  augment() |>\n",
    "  ggplot(aes(x = Beer, y = .fitted,\n",
    "             colour = factor(quarter(Quarter)))) +\n",
    "  geom_point() +\n",
    "  labs(y = \"Fitted\", x = \"Actual values\",\n",
    "       title = \"Australian quarterly beer production\") +\n",
    "  geom_abline(intercept = 0, slope = 1) +\n",
    "  guides(colour = guide_legend(title = \"Quarter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef0b8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distributed lags\n",
    "\n",
    "It is often useful to include advertising expenditure as a predictor. However, since the effect of advertising can last beyond the actual campaign, we need to include lagged values of advertising expenditure. Thus, the following predictors may be used. \n",
    "\n",
    "\\begin{align*}\n",
    "  x_{1} &= \\text{advertising for previous month;} \\\\\n",
    "  x_{2} &= \\text{advertising for two months previously;} \\\\\n",
    "        & \\vdots \\\\\n",
    "  x_{m} &= \\text{advertising for $m$ months previously.}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb5b05",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## \"Irregular\" holidays\n",
    "\n",
    "Easter differs from most holidays because it is not held on the same date each year, and its effect can last for several days. In this case, a dummy variable can be used with value one where the holiday falls in the particular time period and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc491108",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting predictors\n",
    "\n",
    "When there are many possible predictors, we need some strategy for selecting the best predictors to use in a regression model.\n",
    "\n",
    "How to choose? Look at measures of predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ddf3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: US consumption expenditure\n",
    "\n",
    "fit_consMR <- us_change |>\n",
    "  model(model.1 = TSLM(Consumption ~ Income),\n",
    "        model.2 = TSLM(Consumption ~ Income + Savings),\n",
    "        model.3 = TSLM(Consumption ~ Income + Production + Unemployment + Savings))\n",
    "\n",
    "fit_consMR |>\n",
    "    glance() |>\n",
    "    select(.model, adj_r_squared, CV, AIC, AICc, BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeed8eb",
   "metadata": {},
   "source": [
    "For the CV, AIC, AICc and BIC measures, we want to find the model with the lowest value.\n",
    "For Adjusted R2, we seek the model with the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519cdf5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Adjusted $R^2$\n",
    "\n",
    "$$\\bar{R}^2 = 1-(1-R^2)\\frac{T-1}{T-k-1},$$\n",
    "\n",
    "where $T$ is the number of observations and $k$ is the number of predictors. \n",
    "\n",
    "This is an improvement on $R^2$, as it does not necessarily increases with each added predictor. \n",
    "\n",
    "Maximising $\\bar{R}^2$ works quite well as a method of selecting predictors, although it does tend to err on the side of selecting too many predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063eefc8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "The procedure uses the following steps:\n",
    "\n",
    "1. Remove observation $t$ from the data set, and fit the model using the remaining data. Then compute the error $(e^∗_t=y_t−\\hat{y}_t$) for the omitted observation. \n",
    "2. Repeat step 1 for $t=1,\\ldots,T$\n",
    "3. Compute the MSE from $e^∗_1, \\ldots, ,e^*_T$. We shall call this the CV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11575dd6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: US consumption\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/table_cv.png\" alt=\"Table CV\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374af1f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stepwise regression\n",
    "\n",
    "If there are a large number of predictors, it is not possible to fit all possible models.\n",
    "For example, 40 predictors leads to $2^{40}$ > 1 trillion possible models! \n",
    "\n",
    "Consequently, a strategy is required to limit the number of models to be explored.\n",
    "\n",
    "An approach that works quite well is backwards stepwise regression:\n",
    "\n",
    "- Start with the model containing all potential predictors.\n",
    "- Remove one predictor at a time. Keep the model if it improves the measure of predictive accuracy.\n",
    "- Iterate until no further improvement.\n",
    "\n",
    "If the number of potential predictors is too large, then the backwards stepwise regression will not work and forward stepwise regression can be used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30fa46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forecasting with regression\n",
    "\n",
    "Recall that predictions of $y$ can be obtained using:\n",
    "$$\\hat{y_t} = \\hat\\beta_{0} + \\hat\\beta_{1} x_{1,t} + \\hat\\beta_{2} x_{2,t} + \\cdots + \\hat\\beta_{k} x_{k,t}$$\n",
    "\n",
    "What we are interested in here, however, is forecasting **future values** of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5f041",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ex-ante versus ex-post forecasts\n",
    "\n",
    "**Ex-ante forecasts**\n",
    "Made using only the information that is available in advance. \n",
    "\n",
    "These are genuine forecasts, made in advance using whatever information is available at the time. \n",
    "\n",
    "Therefore in order to generate ex-ante forecasts, the model requires forecasts of the predictors. \n",
    "  \n",
    "**Ex-post forecasts**\n",
    "Made using later information on the predictors. These are not genuine forecasts, but are useful for studying the behaviour of forecasting models.\n",
    " \n",
    " The model from which ex-post forecasts are produced should not be estimated using data from the forecast period. That is, ex-post forecasts can assume knowledge of the predictor variables (the $x$ variables), but should not assume knowledge of the data that are to be forecast (the $y$ variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f2c6f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scenario based forecasting\n",
    "\n",
    "In this setting, the forecaster assumes possible scenarios for the predictor variables that are of interest. For example, a US policy maker may be interested in comparing the predicted change in consumption when there is a constant growth of 1% and 0.5% respectively for income and savings with no change in the employment rate, versus a respective decline of 1% and 0.5%, for each of the four quarters following the end of the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d344f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_consBest <- us_change |>\n",
    "  model(\n",
    "    lm = TSLM(Consumption ~ Income + Savings + Unemployment)\n",
    "  )\n",
    "\n",
    "fit_consBest |> report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be4b1fab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "future_scenarios <- scenarios(\n",
    "  Increase = new_data(us_change, 4) |>\n",
    "    mutate(Income=1, Savings=0.5, Unemployment=0),\n",
    "  Decrease = new_data(us_change, 4) |>\n",
    "    mutate(Income=-1, Savings=-0.5, Unemployment=0),\n",
    "  names_to = \"Scenario\")\n",
    "\n",
    "future_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "058f82b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fc <- fit_consBest |> forecast(new_data = future_scenarios)\n",
    "\n",
    "us_change |>\n",
    "  autoplot(Consumption) +\n",
    "  autolayer(fc) +\n",
    "  labs(title = \"US consumption\", y = \"% change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf776dd0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building a predictive regression model\n",
    "\n",
    "An alternative formulation is to use as predictors their lagged values. Assuming that we are interested in generating a h-step ahead forecast we write \n",
    "$$y_{t+h}=\\beta_0+\\beta_1x_{1,t}+\\dots+\\beta_kx_{k,t}+\\varepsilon_{t+h}$$\n",
    "for $h=1,2, \\ldots$ \n",
    "\n",
    "Including lagged values of the predictors does not only make the model operational for easily generating forecasts, it also makes it intuitively appealing (no simultaneous effects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31e24ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitc <- us_change |>\n",
    "  model(\n",
    "    lm = TSLM(Consumption ~ Income + Savings + Unemployment),\n",
    "    lm_lag_1 = TSLM(Consumption ~ lag(Income) + lag(Savings) + lag(Unemployment)),\n",
    "    lm_lag_2 = TSLM(Consumption ~ lag(Income)),\n",
    "    lm_lag_3 = TSLM(Consumption ~ lag(Consumption) + lag(Income))\n",
    "  )\n",
    "\n",
    "fitc |> glance() |> select(.model, adj_r_squared, CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6366487",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Guided Workbook (part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f9f16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear regression\n",
    "\n",
    "Although the linear relationship assumed so far in this chapter is often adequate, there are many cases in which a nonlinear functional form is more suitable. To keep things simple in this section we assume that we only have one predictor $x$.\n",
    "\n",
    "The simplest way of modelling a nonlinear relationship is to transform the forecast variable $y$ and/or the predictor variable $x$ before estimating a regression model.\n",
    "\n",
    "For example:\n",
    "$$y=f(x) +\\varepsilon$$\n",
    "where $f$ is a nonlinear function.\n",
    "\n",
    "Alternative example. A log-log functional form is specified as \n",
    "$$\\log y=\\beta_0+\\beta_1 \\log x +\\varepsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f2c56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Boston marathon winning times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dc16ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_men <- boston_marathon |>\n",
    "  filter(Year >= 1924) |>\n",
    "  filter(Event == \"Men's open division\") |>\n",
    "  mutate(Minutes = as.numeric(Time)/60)\n",
    "\n",
    "boston_men |>\n",
    "    autoplot(Minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9a691",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Piecewise linear transformations\n",
    "\n",
    "Introduce points where the slope of $f$ can change. These points are called knots. This can be achieved by letting $x_1=x$ and introducing variable $x_2$ such that \n",
    "\n",
    "\\begin{align*}\n",
    "  x_{2} = (x-c)_+ &= \\left\\{\n",
    "             \\begin{array}{ll}\n",
    "               0 & \\text{if } x < c\\\\\n",
    "               x-c &  \\text{if } x \\ge c.\n",
    "             \\end{array}\\right.\n",
    "\\end{align*}\n",
    "\n",
    "The notation $(x−c)_+$ means the value $x−c$ if it is positive and 0 otherwise.\n",
    "\n",
    "Piecewise linear relationships constructed in this way are a special case of regression splines. In general, a linear regression spline is obtained using \n",
    "$$x_{1}= x \\quad x_{2} = (x-c_{1})_+ \\quad\\dots\\quad x_{k} = (x-c_{k-1})_+$$\n",
    "\n",
    "Selecting the number of knots $(k−1)$ and where they should be positioned can be difficult and somewhat arbitrary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3dd15d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forecasting with a nonlinear trend\n",
    "\n",
    "Polynomial transformations:\n",
    "\n",
    "$$x_{1,t} =t,\\quad x_{2,t}=t^2,\\quad \\dots.$$\n",
    "\n",
    "However, it is not recommended that quadratic or higher order trends be used in forecasting. When they are extrapolated, the resulting forecasts are often unrealistic.\n",
    "\n",
    "A better approach is to use the piecewise specification introduced above and fit a piecewise linear trend which bends at some point in time. We can think of this as a nonlinear trend constructed of linear pieces. If the trend bends at time $\\tau$, then it can be specified by simply replacing $x=t$ and $c=\\tau$ above such that we include the predictors, \n",
    "\\begin{align*}\n",
    "  x_{1,t} & = t \\\\\n",
    "  x_{2,t} &= (t-\\tau)_+ = \\left\\{\n",
    "             \\begin{array}{ll}\n",
    "               0 & \\text{if } t < \\tau\\\\\n",
    "               t-\\tau &  \\text{if } t \\ge \\tau\n",
    "             \\end{array}\\right.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1e099",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Boston marathon winning times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63367a3c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "boston_men <- boston_marathon |>\n",
    "  filter(Year >= 1924) |>\n",
    "  filter(Event == \"Men's open division\") |>\n",
    "  mutate(Minutes = as.numeric(Time)/60)\n",
    "\n",
    "boston_men |>\n",
    "    autoplot(Minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bed9f4b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fit_trends <- boston_men |>\n",
    "  model(\n",
    "    linear = TSLM(Minutes ~ trend()),\n",
    "    exponential = TSLM(log(Minutes) ~ trend()),\n",
    "    piecewise = TSLM(Minutes ~ trend(knots = c(1950, 1980)))\n",
    "  )\n",
    "\n",
    "fc_trends <- fit_trends |> forecast(h = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a911cf43",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "boston_men |>\n",
    "  autoplot(Minutes) +\n",
    "  geom_line(data = fitted(fit_trends),\n",
    "            aes(y = .fitted, colour = .model)) +\n",
    "  autolayer(fc_trends, alpha = 0.5, level = 95) +\n",
    "  labs(y = \"Minutes\",\n",
    "       title = \"Boston marathon winning times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a3386",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation is not causation\n",
    "\n",
    "It is important not to confuse correlation with causation, or causation with forecasting. A variable $x$ may be useful for forecasting a variable y, but that does not mean $x$ is causing $y$. \n",
    "\n",
    "It is possible that $x$ is causing $y$, but it may be that $y$ is causing $x$, or that the relationship between them is more complicated than simple causality.\n",
    "\n",
    "For example, it is possible to model the number of drownings at a beach resort each month with the number of ice-creams sold in the same period. \n",
    "\n",
    "Similarly, it is possible to forecast if it will rain in the afternoon by observing the number of cyclists on the road in the morning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693a5ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spurious correlations\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/5825_google-searches-for-taylor-swift_correlates-with_fossil-fuel-use-in-british-virgin-islands.svg\" alt=\"Taylor Swift\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "Credit: http://www.tylervigen.com/spurious-correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e25a5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forecasting with correlated predictors\n",
    "\n",
    "When two or more predictors are highly correlated it is always challenging to accurately separate their individual effects.\n",
    "\n",
    "Suppose we are forecasting monthly sales of a company for 2012, using data from 2000–2011. In January 2008, a new competitor came into the market and started taking some market share. At the same time, the economy began to decline. In your forecasting model, you include both competitor activity (measured using advertising time on a local television station) and the health of the economy (measured using GDP). It will not be possible to separate the effects of these two predictors because they are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fbaec1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multicollinearity and forecasting\n",
    "\n",
    "A closely related issue is multicollinearity, which occurs when similar information is provided by two or more of the predictor variables in a multiple regression.\n",
    "\n",
    "It can occur when two predictors are highly correlated with each other (that is, they have a correlation coefficient close to +1 or -1). In this case, knowing the value of one of the variables tells you a lot about the value of the other variable.\n",
    "\n",
    "When multicollinearity is present, the uncertainty associated with individual regression coefficients will be large. This is because they are difficult to estimate. Consequently, statistical tests (e.g., t-tests) on regression coefficients are unreliable. (In forecasting we are rarely interested in such tests.) Also, it will not be possible to make accurate statements about the contribution of each separate predictor to the forecast.\n",
    "\n",
    "It is always a little dangerous when future values of the predictors lie much outside the historical range, but it is especially problematic when multicollinearity is present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1929c45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Guided Workbook (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5386c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Time\n",
    "\n",
    "- Exponential smoothing\n",
    "- Practice midterm (1.5 hours)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
